compile:
  backend: inductor
data:
  drop_last: true
  num_workers: 0
  pad_direction: right
  paths:
  - /mnt/geogpt-gpfs/llm-course/public/tianwen/npy/
  - /test.npy
  persistent_workers: true
  pin_memory: true
  prefetch_factor: 16
  timeout: 0
device_eval_batch_size: ${device_train_microbatch_size}
device_train_microbatch_size: 8
dry_run: false
eval_interval: ${save_interval}
eval_subset_num_batches: -1
fsdp:
  precision: mixed
  sharding_strategy: SHARD_GRAD_OP
  wrapping_strategy: by_block
global_train_batch_size: 2048
load_path: null
max_duration: 739328
max_grad_norm: 1.0
max_grad_norm_ratio: null
model:
  activation_type: swiglu
  alibi: false
  attention_dropout: 0.0
  attention_layer_norm: false
  attention_layer_norm_with_affine: false
  bias_for_layer_norm: false
  block_type: sequential
  d_model: 2048
  embedding_dropout: 0.0
  embedding_size: 50304
  eos_token_id: 0
  flash_attention: true
  include_bias: false
  init_device: meta
  init_fn: mitchell
  layer_norm_type: default
  layer_norm_with_affine: false
  max_sequence_length: 2048
  mlp_ratio: 8
  multi_query_attention: false
  n_heads: 16
  n_layers: 32
  pad_token_id: 0
  residual_dropout: 0.0
  rope: true
  vocab_size: 50280
  weight_tying: false
optimizer:
  betas:
  - 0.9
  - 0.95
  learning_rate: 0.0004
  metrics_log_interval: 10
  name: adamw
  weight_decay: 0.1
precision: amp_bf16
run_name: OLMo-2B-stella
save_folder: ${path.choose:${oc.env:SCRATCH_DIR,no_exist}/checkpoints,/results}/${oc.env:SLURM_JOB_ID,${run_name}}
save_interval: 1000
save_interval_unsharded: 10000
save_num_checkpoints_to_keep: 15
save_num_unsharded_checkpoints_to_keep: -1
save_overwrite: true
scheduler:
  alpha_f: 0.1
  name: cosine_with_warmup
  t_warmup: 2000
seed: 3407
sharded_checkpointer: torch_legacy
speed_monitor:
  window_size: 20
time_limit: 1209600.0
tokenizer:
  identifier: /mnt/geogpt-gpfs/llm-course/home/zhangjianxing/hengxing_2B/configs/allenai_eleuther-ai-gpt-neox-20b-pii-special.json
  truncate_direction: right
