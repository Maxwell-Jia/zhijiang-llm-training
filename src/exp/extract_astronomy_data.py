"""
Script for extracting documents similar to astronomy or physics titles using sentence embeddings.

This script takes astronomy or physics titles generated by GPT-4, and a directory containing JSON files of documents.
It calculates the similarity between the titles and document titles using a pre-trained sentence embeddings model.
Documents with a similarity score above a specified threshold are saved to a JSONL file.
"""

import os
from sentence_transformers import SentenceTransformer
import gzip
import json
import torch
import argparse
import time
from tqdm import tqdm

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--astro_title_path", default=None, type=str, required=True)
    parser.add_argument("--document_dir", default=None, type=str, required=True)
    parser.add_argument("--similar_threshold", default=0.8, type=float)
    parser.add_argument("--output_path", default=None, type=str, required=True)
    parser.add_argument("--max_items", default=20, type=int)
    parser.add_argument("--root", default="/mnt/geogpt-gpfs/llm-course/public/datasets/", type=str)
    args = parser.parse_args()

    device = torch.device("cuda:7")

    # Load sentence embeddings model    
    model = SentenceTransformer('flax-sentence-embeddings/all_datasets_v4_MiniLM-L6', device=device)

    # Load astronomy or physics titles generated by gpt4
    with open(args.astro_title_path, 'r') as f:
        astro_title_list = f.read().splitlines()

    # Embedding astronomy titles
    target_embeddings = model.encode(astro_title_list, show_progress_bar=False, batch_size=1024, convert_to_numpy=False, normalize_embeddings=True, device=device)
    target_embeddings = torch.vstack(target_embeddings)

    # Get list of .json.gz files in the directory
    document_dir = os.path.join(args.root, args.document_dir)
    file_list = os.listdir(document_dir)
    # file_list.sort()

    item_count = 0
    for file in file_list:
        # Load .json.gz file
        print("Processing file: ", file)
        print("Loading file...")
        start_time = time.time()
        with gzip.open(os.path.join(document_dir, file), 'rt') as f:
            title_list = []
            id_list = []
            text_list = []
            # for line in tqdm(f):
            for line in f:
                doc = json.loads(line)
                title_list.append(doc['metadata']['title'])
                id_list.append(doc['id'])
                text_list.append(doc['text'])
        print("File Loaded.")
        
        # Embedding document titles
        source_embeddings = model.encode(title_list, show_progress_bar=False, batch_size=1024, convert_to_numpy=False, normalize_embeddings=True, device=device)
        source_embeddings = torch.vstack(source_embeddings)

        # Calculate the similarity matrix of astronomy titles and dolma titles.
        # Columns of the matrix represents dolma titles, and the rows represent astronomy titles.
        title_similar_matrix = torch.matmul(target_embeddings, source_embeddings.T)
        source_max_similar = title_similar_matrix.max(dim=0).values

        idx_over_threshold = torch.where(source_max_similar >= args.similar_threshold)[0].cpu().numpy()
        
        # Save to jsonl file
        with open(args.output_path, 'a') as f:
            for idx in idx_over_threshold:
                f.write(json.dumps({
                    'id': id_list[idx],
                    'source': os.path.join(args.document_dir, file),
                    'text': text_list[idx]
                }) + '\n')

        print(f"Load {idx_over_threshold.shape[0]} items from {file}. Current totle: {item_count}")
        end_time = time.time()
        print(f"Time cost: {end_time - start_time} seconds for {file}")
        item_count += idx_over_threshold.shape[0]
        if item_count >= args.max_items:
            print("Maximum number of items reached.")
            break

    if item_count < args.max_items:
        print(f"Only {item_count} items were extracted.")


if __name__ == '__main__':
    main()